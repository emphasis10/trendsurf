# IMPLEMENTATION_PLAN.md — TrendSurf

> Scope: Implement Phase 1 (MVP) of a web tool that tracks topic‑specific paper trends for researchers, with **GROBID full‑text parsing from Day 1**, **Qdrant** as the vector DB, **Ollama + OpenAI‑compatible** generation backends (user‑selectable), and a **single system‑wide embedding model**.

---

## 0) Goals & Non‑Goals

**Goals**
- Ingest papers from arXiv by user topics (filters & keywords), **respecting arXiv rate limits**.
- For each paper: fetch metadata + PDF, parse full text via **GROBID** (TEI/XML), embed text once using the **system embedding** model, rank per topic, produce **TL;DR**, **structured AI analysis**, and a **novelty score (0–10)**.
- Let each user pick their **generation provider/model** (Ollama or OpenAI‑compatible). Embedding model is **system‑wide** (read‑only in UI).
- Deliver a clean **Next.js** UI + **FastAPI** backend + background workers.
- Deploy with Docker Compose. (**Compose file will be generated by codex‑cli as part of this plan**.)

**Non‑Goals (Phase 1)**
- Daily digest notifications (email/Slack) — deferred.
- Cross‑provider embedding selection by end users.
- Non‑arXiv sources (Crossref/Semantic Scholar/PWC) — design hooks only.

---

## 1) Milestones

**M0 — Scaffolding & Infra (2–3 days)**
- Repo init, base services, shared types, environment handling.
- DB schema migrations (Postgres), Qdrant collections, provider clients.
- codex‑cli tasks to generate `docker-compose.yml` + `.env.sample`.

**M1 — Ingestion & Parsing (3–5 days)**
- arXiv fetcher with idempotent upserts & polite rate limiting.
- PDF downloader with retry/cache.
- **GROBID** integration (processFulltextDocument → TEI). Minimal TEI → text normalization pipeline.

**M2 — Embedding, Matching, Analysis (3–5 days)**
- Embedding client (system‑wide model) + Qdrant upserts.
- Topic anchor creation + vector matching (top‑k + filters).
- Summarizer + novelty scorer (hybrid heuristic + LLM critique).

**M3 — UI & Auth (3–4 days)**
- Auth (JWT + refresh cookies), topic manager, dashboard feed, detail page.
- Settings page for **generation** provider/model selection (read‑only embedding model).

**M4 — QA, E2E, Basic Observability (2–3 days)**
- Unit tests for algorithms & prompts schema validation.
- Telemetry (logs/metrics/health) & SLO checks.

---

## 2) Repository Structure

```
trendsurf/
  apps/
    api/               # FastAPI app
      src/
        core/          # settings, logging, errors
        auth/          # JWT, password hashing, deps
        providers/     # genClient, embedClient (ollama & openai‑compat)
        ingestion/     # arxiv client, pdf fetcher, rate limiter
        parsing/       # grobid client, TEI→normalized text
        vectors/       # qdrant client, collections, upsert/query
        matching/      # topic anchors, rankers
        analysis/      # tldr, analysis, novelty hybrid scorer
        routers/       # FastAPI routers
        models/        # SQLAlchemy models (Postgres)
        schemas/       # Pydantic DTOs
        tasks/         # rq workers task definitions
      tests/
      pyproject.toml
    frontend/          # Next.js app
      app/             # routes
      components/
      lib/
      tests/
    worker/            # RQ workers entrypoint (shares api/src lib)
  deploy/
    compose/           # codex‑cli will generate docker‑compose.yml here
  prompts/
    tldr.v1.json
    analysis.v1.json
    novelty_critic.v1.json
  scripts/
    bootstrap.sh
  .env.sample          # codex‑cli generates/updates
  README.md
```

---

## 3) Services & Responsibilities

**Frontend (Next.js)**
- Auth pages, topic manager, dashboard feed (per topic), paper detail.
- Settings: update **generation provider/model**; show embedding model (read‑only).

**API (FastAPI)**
- REST endpoints (see §9), JWT auth, validation, rate‑limit (ingest)
- Orchestrates arXiv fetch, PDF download, **GROBID** call, vector upsert, analysis tasks.

**Worker (RQ + Redis)**
- Runs heavy jobs: parse PDF, embed, match, summarize, novelty score.
- Retry, backoff, dead‑letter queue.

**Data Stores**
- **Postgres** for relational metadata and analysis outputs.
- **Qdrant** for vectors (`paper_vectors`, `topic_vectors`).

**External**
- **GROBID** service container (port 8070): `/api/processFulltextDocument`.
- **Ollama** (optional local) for generation + embeddings (if chosen as embed provider).
- **OpenAI‑compatible** base URL & key for generation (and optionally embeddings if selected as system model).

---

## 4) Data Model (SQL outline)

```sql
-- users, topics, papers, analyses, topic_matches, settings, jobs
CREATE TABLE users (
  id BIGSERIAL PRIMARY KEY,
  email TEXT UNIQUE NOT NULL,
  name TEXT,
  password_hash TEXT NOT NULL,
  role TEXT DEFAULT 'user',
  llm_provider TEXT,         -- 'ollama' | 'openai_compat'
  llm_model TEXT,            -- e.g., 'llama3.1:8b' or 'gpt-4o-mini'
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

CREATE TABLE topics (
  id BIGSERIAL PRIMARY KEY,
  user_id BIGINT REFERENCES users(id) ON DELETE CASCADE,
  name TEXT NOT NULL,
  description TEXT,
  filters_json JSONB NOT NULL DEFAULT '{}', -- arXiv cats, keywords
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

CREATE TABLE papers (
  id BIGSERIAL PRIMARY KEY,
  source TEXT NOT NULL,         -- 'arxiv'
  source_id TEXT NOT NULL,      -- arXiv id
  title TEXT NOT NULL,
  authors TEXT[],
  abstract TEXT,
  published_at TIMESTAMPTZ,
  updated_at TIMESTAMPTZ,
  url_pdf TEXT,
  url_page TEXT,
  primary_category TEXT,
  meta_json JSONB DEFAULT '{}',
  UNIQUE (source, source_id)
);

CREATE TABLE analyses (
  id BIGSERIAL PRIMARY KEY,
  paper_id BIGINT REFERENCES papers(id) ON DELETE CASCADE,
  tldr JSONB,                  -- { bullets: [] }
  ai_summary JSONB,            -- { method, data, results, limitations, impact }
  novelty_score NUMERIC(3,1),
  novelty_rationale TEXT,
  provider TEXT,               -- 'ollama' | 'openai_compat'
  gen_model TEXT,
  embed_model TEXT,
  tokens INT,
  latency_ms INT,
  status TEXT,
  error TEXT,
  created_at TIMESTAMPTZ DEFAULT now()
);

CREATE TABLE topic_matches (
  topic_id BIGINT REFERENCES topics(id) ON DELETE CASCADE,
  paper_id BIGINT REFERENCES papers(id) ON DELETE CASCADE,
  score REAL NOT NULL,
  reason TEXT,
  created_at TIMESTAMPTZ DEFAULT now(),
  PRIMARY KEY(topic_id, paper_id)
);

CREATE TABLE settings (
  key TEXT PRIMARY KEY,
  value_json JSONB NOT NULL,
  scope TEXT NOT NULL -- 'system' or 'user:{id}'
);

CREATE TABLE jobs (
  id BIGSERIAL PRIMARY KEY,
  name TEXT,
  last_run_at TIMESTAMPTZ,
  state JSONB
);
```

**Vectors** (Qdrant)
- `paper_vectors` collection: payload `{ paper_id, model_name, dim, primary_category, published_at }`
- `topic_vectors` collection: payload `{ topic_id, user_id }`

---

## 5) Provider Abstraction

**Generation (user‑selectable)**
- `OLLAMA_BASE_URL`, default `OLLAMA_GEN_MODEL` per user; use OpenAI‑compatible Chat API when `openai_compat`.

**Embeddings (system‑wide)**
- `EMBED_PROVIDER` in { `ollama`, `openai_compat` }
- `EMBED_BASE_URL`, `EMBED_API_KEY` (if needed), `EMBED_MODEL`

Clients expose a uniform interface:
```python
summary = gen_client.chat(messages=[...], model=user.gen_model)
vec = embed_client.embed(texts=[...])
```

---

## 6) Ingestion Pipeline (Idempotent)

1. **Topic expansion → query**
   - Build arXiv queries from `filters_json` (cats + keywords, date window).
2. **Fetch candidates (polite)**
   - Single connection, ≥3s delay between requests; pagination; retries with jitter.
3. **Upsert papers**
   - Insert new or update changed metadata by `(source, source_id)`.
4. **PDF download**
   - Resolve `url_pdf`; conditional GET; store to temp; checksum to avoid duplicates.
5. **GROBID parse**
   - POST to `/api/processFulltextDocument` with PDF; store TEI (optional) and normalized plain text.
6. **Embed**
   - Select text budget (abstract + capped body); create vector; upsert to **Qdrant**.
7. **Match**
   - Ensure topic anchor vectors exist (embed description+keywords once); search top‑k; write `topic_matches`.
8. **Analyze**
   - Queue TL;DR + analysis; compute novelty (hybrid); persist to `analyses`.

**Retry/Backoff**: Exponential; dead‑letter queue table with minimal payload.

---

## 7) Novelty Score (Hybrid 0–10)

- **Algorithmic (0–7)**
  - Semantic distance vs. last 12‑month corpus (3)
  - Keyword delta via BM25/KeyBERT vs. topic centroid (2)
  - Citation overlap Jaccard vs. recent cluster (2) — if references available from TEI
- **LLM Critique (0–3)**
  - `novelty_critic.v1` prompt → score & rationale
- Confidence band (`low/med/high`) based on available text and critique agreement.

---

## 8) Prompts (JSON‑first)

- `prompts/tldr.v1.json`: 3–5 bullet TL;DR
- `prompts/analysis.v1.json`: { method, data, results, limitations, impact }
- `prompts/novelty_critic.v1.json`: originality 0–3 + rationale

All LLM outputs must be **validated** against JSON schemas; on failure retry with a function‑style system nudge.

---

## 9) API Contract (sketch)

```
POST /auth/register | /auth/login
GET  /me                      -> { email, name, llm_provider, llm_model }
PATCH /me                     -> (update llm_provider, llm_model)

GET  /topics                  -> list
POST /topics                  -> create
PATCH /topics/:id             -> update
DELETE /topics/:id            -> delete

GET  /feed?topic_id=&cursor=  -> { items: [{paper, analysis, match_score}], nextCursor }
GET  /papers/:id              -> { paper, analysis, related }
GET  /settings/embedding      -> { provider, model }  # read‑only

POST /admin/reindex (authz: admin)
```

---

## 10) Config & ENV

- Core: `JWT_SECRET`, `CORS_ORIGIN`, `TZ=Asia/Seoul`
- DB: `POSTGRES_DB`, `POSTGRES_USER`, `POSTGRES_PASSWORD`, `DATABASE_URL`
- Redis: `REDIS_URL`
- Qdrant: `QDRANT_URL`, `QDRANT_API_KEY`, collection names
- GROBID: `GROBID_URL` (e.g., `http://grobid:8070`), `GROBID_TIMEOUT`
- Providers:
  - Gen: `LLM_ALLOWLIST`, `OLLAMA_BASE_URL`, `OPENAI_BASE_URL`, `OPENAI_API_KEY`
  - Embed: `EMBED_PROVIDER`, `EMBED_BASE_URL`, `EMBED_API_KEY`, `EMBED_MODEL`
- Ingest: `ARXIV_DELAY_MS=3200`, `ARXIV_MAX_RESULTS_PER_RUN`, `PDF_MAX_MB`, `USER_AGENT`

`.env.sample` will contain safe placeholders; secrets excluded from VCS.

---

## 11) Observability & Health

- **Liveness/Readiness** endpoints on API/Worker.
- Probes: **GROBID** `/api/isalive`, **Qdrant** `/livez`, **Postgres** simple query.
- Structured logs (JSON) + basic counters (requests, failures, avg latency).

---

## 12) Testing Strategy

- **Unit**: parsing normalizer, novelty components, prompt validators, arXiv client w/ fake clock.
- **Integration**: end‑to‑end ingest for a small fixture set (N=3 PDFs), asserting TL;DR schema, Qdrant writes, feed ordering.
- **Contract**: OpenAPI schema for backend; Playwright for key UI flows.

---

## 13) Security & Privacy

- Hash passwords (argon2/bcrypt). Short‑lived JWT + httpOnly refresh cookie.
- Server‑side provider keys only. Per‑user gen settings whitelisted by server.
- Respect robots/ToU of sources; store only necessary PII.

---

## 14) Performance & Sizing Notes

- GROBID full image is large and memory‑hungry; allocate sufficient RAM. If constrained, switch to CRF image in ops (no code change).
- ArXiv requests are serialized with ≥3s delay; worker concurrency targets PDF/LLM tasks, not arXiv.
- Limit embedding text budget (e.g., 6–8k chars) with section sampling.

---

## 15) codex‑cli Tasks (authoritative for automation)

> **Important:** `docker-compose.yml` and `.env.sample` are **generated by codex‑cli** using these tasks. Adjust names to your local codex‑cli syntax.

### T0 — Project bootstrap
- Create monorepo structure (see §2).
- Initialize Python (API/Worker) + Node (Next.js) workspaces.
- Generate shared `pyproject.toml`, `package.json`, lint/format configs.

### T1 — Compose & ENV generation (codex must write files)
- **Generate `deploy/compose/docker-compose.yml`** with services:
  - `api` (FastAPI, port 8000)
  - `worker` (RQ workers)
  - `frontend` (Next.js, port 3000)
  - `postgres` (with healthcheck)
  - `redis`
  - `qdrant` (with volumes, healthcheck)
  - `grobid` (image tag configurable; map 8070)
  - `ollama` (optional; expose 11434)
- Networks: single `trendsurf` network; volumes for `postgres`, `qdrant`, `ollama` models, and optional `grobid` cache.
- Healthchecks:
  - api: `GET /healthz`
  - qdrant: `GET /livez`
  - grobid: `GET /api/isalive`
  - postgres: `pg_isready`
- **Generate `.env.sample`** with all keys listed in §10.

### T2 — DB & Qdrant init
- Create Alembic migrations from §4 schema.
- Create Qdrant collections with payload schemas and HNSW params.

### T3 — Providers
- Implement `providers/gen_client.py` (Ollama & OpenAI‑compat) and `providers/embed_client.py` (system‑wide).
- Add allowlist validation for user‑set gen models.

### T4 — Ingestion & Parsing
- Implement arXiv client with 3.2s delay, single connection, robust pagination.
- Implement PDF downloader.
- Implement **GROBID** client (multipart → TEI) + TEI→text normalizer.

### T5 — Embedding & Matching
- Implement Qdrant upsert/query. Build topic‑anchor vectors. Rank papers per topic with filters.

### T6 — Analysis & Novelty
- Wire TL;DR + analysis prompts; JSON validation w/ retry. Hybrid novelty score.

### T7 — API/Frontend
- Implement endpoints (§9). Next.js pages: Auth, Topics, Dashboard, Paper detail, Settings.

### T8 — Tests & CI
- Unit/integration tests; minimal CI workflow (lint, test, build images).

---

## 16) Rollout Checklist

- [ ] `docker-compose.yml` and `.env.sample` generated by **codex‑cli**
- [ ] `docker compose up -d` brings up all services; healthchecks pass
- [ ] First run bootstrap: create system embedding setting; create admin user
- [ ] Seed topics for test users; run `cron:ingest` once; verify UI feed

---

## 17) Future Work (post‑MVP)
- Digest emails/Slack, Multi‑source ingestion (Crossref/S2/PWC), PDF caching, GPU‑enabled GROBID image, per‑topic advanced filters, richer novelty explainability.

---

**End of IMPLEMENTATION_PLAN.md**

